{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Exploratory Data Analysis (EDA)\n",
    "## Debt Collection ML System\n",
    "\n",
    "This notebook provides in-depth exploratory data analysis for the debt collection dataset, focusing on understanding patterns that influence repayment behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.data_generator import DebtCollectionDataGenerator\n",
    "\n",
    "# Generate or load data\n",
    "if os.path.exists('../data/raw/debt_collection_data.csv'):\n",
    "    df = pd.read_csv('../data/raw/debt_collection_data.csv')\n",
    "    print(\"Loaded existing data\")\n",
    "else:\n",
    "    generator = DebtCollectionDataGenerator(n_samples=10000)\n",
    "    df = generator.generate_dataset()\n",
    "    \n",
    "    # Save data\n",
    "    os.makedirs('../data/raw', exist_ok=True)\n",
    "    df.to_csv('../data/raw/debt_collection_data.csv', index=False)\n",
    "    print(\"Generated new data\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "outcome_counts = df['Outcome'].value_counts()\n",
    "outcome_pct = df['Outcome'].value_counts(normalize=True) * 100\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Outcome Distribution (Count)', 'Outcome Distribution (Percentage)'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}]]\n",
    ")\n",
    "\n",
    "# Bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=outcome_counts.index, y=outcome_counts.values, name='Count'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Pie chart\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=outcome_pct.index, values=outcome_pct.values, name='Percentage'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Target Variable Analysis\")\n",
    "fig.show()\n",
    "\n",
    "print(\"Outcome Distribution:\")\n",
    "for outcome, count in outcome_counts.items():\n",
    "    print(f\"{outcome}: {count} ({outcome_pct[outcome]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by outcome\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Age distribution\n",
    "for i, outcome in enumerate(df['Outcome'].unique()):\n",
    "    subset = df[df['Outcome'] == outcome]\n",
    "    axes[0, 0].hist(subset['Age'], alpha=0.7, label=outcome, bins=30)\n",
    "axes[0, 0].set_title('Age Distribution by Outcome')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Income distribution (log scale)\n",
    "for i, outcome in enumerate(df['Outcome'].unique()):\n",
    "    subset = df[df['Outcome'] == outcome]\n",
    "    axes[0, 1].hist(np.log10(subset['Income']), alpha=0.7, label=outcome, bins=30)\n",
    "axes[0, 1].set_title('Income Distribution by Outcome (Log Scale)')\n",
    "axes[0, 1].set_xlabel('Log10(Income)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Occupation distribution\n",
    "occupation_outcome = pd.crosstab(df['Occupation'], df['Outcome'], normalize='index') * 100\n",
    "occupation_outcome.plot(kind='bar', ax=axes[1, 0], stacked=True)\n",
    "axes[1, 0].set_title('Outcome Distribution by Occupation')\n",
    "axes[1, 0].set_ylabel('Percentage')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Region distribution\n",
    "region_outcome = pd.crosstab(df['Region'], df['Outcome'], normalize='index') * 100\n",
    "region_outcome.plot(kind='bar', ax=axes[1, 1], stacked=True)\n",
    "axes[1, 1].set_title('Outcome Distribution by Region')\n",
    "axes[1, 1].set_ylabel('Percentage')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Financial Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create financial ratios for analysis\n",
    "df_analysis = df.copy()\n",
    "df_analysis['Debt_to_Income_Ratio'] = df_analysis['Outstanding_Balance'] / df_analysis['Income']\n",
    "df_analysis['Loan_Utilization'] = df_analysis['Outstanding_Balance'] / df_analysis['Loan_Amount']\n",
    "\n",
    "# Financial metrics by outcome\n",
    "financial_metrics = ['Outstanding_Balance', 'Loan_Amount', 'Credit_Score', 'Debt_to_Income_Ratio']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, metric in enumerate(financial_metrics):\n",
    "    df_analysis.boxplot(column=metric, by='Outcome', ax=axes[i])\n",
    "    axes[i].set_title(f'{metric} by Outcome')\n",
    "    axes[i].set_xlabel('Outcome')\n",
    "    \n",
    "plt.suptitle('Financial Metrics Analysis', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary by outcome\n",
    "print(\"\\n=== FINANCIAL METRICS BY OUTCOME ===\")\n",
    "for outcome in df['Outcome'].unique():\n",
    "    print(f\"\\n{outcome.upper()}:\")\n",
    "    subset = df_analysis[df_analysis['Outcome'] == outcome]\n",
    "    print(subset[financial_metrics].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Behavioral Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication behavior analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Days Past Due Distribution', 'Response Rate by Outcome', \n",
    "                   'Number of Calls vs Response Rate', 'Contact Channel Effectiveness')\n",
    ")\n",
    "\n",
    "# Days Past Due by outcome\n",
    "for outcome in df['Outcome'].unique():\n",
    "    subset = df[df['Outcome'] == outcome]\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=subset['Days_Past_Due'], name=f'{outcome}', opacity=0.7),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Response rate by outcome\n",
    "response_by_outcome = df.groupby('Outcome')['Response_Rate'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=response_by_outcome.index, y=response_by_outcome.values, name='Avg Response Rate'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Scatter: Number of calls vs Response rate (colored by outcome)\n",
    "for outcome in df['Outcome'].unique():\n",
    "    subset = df[df['Outcome'] == outcome]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=subset['Number_of_Calls'], y=subset['Response_Rate'], \n",
    "                  mode='markers', name=f'{outcome}', opacity=0.6),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Contact channel effectiveness\n",
    "channel_effectiveness = df.groupby(['Last_Contact_Channel', 'Outcome']).size().unstack(fill_value=0)\n",
    "channel_effectiveness_pct = channel_effectiveness.div(channel_effectiveness.sum(axis=1), axis=0) * 100\n",
    "\n",
    "for outcome in channel_effectiveness_pct.columns:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=channel_effectiveness_pct.index, y=channel_effectiveness_pct[outcome], \n",
    "               name=f'{outcome}'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Behavioral Patterns Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != 'Customer_ID']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(\"\\n=== HIGHLY CORRELATED FEATURES (|r| > 0.7) ===\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Risk Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk segments based on multiple factors\n",
    "def create_risk_segments(row):\n",
    "    score = 0\n",
    "    \n",
    "    # Days past due (40% weight)\n",
    "    if row['Days_Past_Due'] > 180:\n",
    "        score += 4\n",
    "    elif row['Days_Past_Due'] > 90:\n",
    "        score += 3\n",
    "    elif row['Days_Past_Due'] > 30:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    # Credit score (30% weight)\n",
    "    if row['Credit_Score'] < 550:\n",
    "        score += 3\n",
    "    elif row['Credit_Score'] < 650:\n",
    "        score += 2\n",
    "    elif row['Credit_Score'] < 750:\n",
    "        score += 1\n",
    "    \n",
    "    # Response rate (20% weight)\n",
    "    if row['Response_Rate'] < 20:\n",
    "        score += 2\n",
    "    elif row['Response_Rate'] < 50:\n",
    "        score += 1\n",
    "    \n",
    "    # Outstanding balance relative to income (10% weight)\n",
    "    debt_ratio = row['Outstanding_Balance'] / row['Income']\n",
    "    if debt_ratio > 0.5:\n",
    "        score += 1\n",
    "    \n",
    "    # Classify risk\n",
    "    if score <= 3:\n",
    "        return 'Low Risk'\n",
    "    elif score <= 6:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'High Risk'\n",
    "\n",
    "df_analysis['Risk_Segment'] = df_analysis.apply(create_risk_segments, axis=1)\n",
    "\n",
    "# Analyze risk segments\n",
    "risk_outcome = pd.crosstab(df_analysis['Risk_Segment'], df_analysis['Outcome'], normalize='index') * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Risk segment distribution\n",
    "risk_counts = df_analysis['Risk_Segment'].value_counts()\n",
    "axes[0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%')\n",
    "axes[0].set_title('Risk Segment Distribution')\n",
    "\n",
    "# Outcome by risk segment\n",
    "risk_outcome.plot(kind='bar', ax=axes[1], stacked=True)\n",
    "axes[1].set_title('Outcome Distribution by Risk Segment')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== RISK SEGMENT ANALYSIS ===\")\n",
    "print(risk_outcome.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time-based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days Past Due analysis\n",
    "dpd_bins = [0, 30, 60, 90, 180, 365, float('inf')]\n",
    "dpd_labels = ['0-30', '31-60', '61-90', '91-180', '181-365', '365+']\n",
    "df_analysis['DPD_Category'] = pd.cut(df_analysis['Days_Past_Due'], bins=dpd_bins, labels=dpd_labels)\n",
    "\n",
    "# Payment behavior by DPD category\n",
    "dpd_outcome = pd.crosstab(df_analysis['DPD_Category'], df_analysis['Outcome'], normalize='index') * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# DPD category distribution\n",
    "dpd_counts = df_analysis['DPD_Category'].value_counts().sort_index()\n",
    "axes[0, 0].bar(range(len(dpd_counts)), dpd_counts.values)\n",
    "axes[0, 0].set_xticks(range(len(dpd_counts)))\n",
    "axes[0, 0].set_xticklabels(dpd_counts.index, rotation=45)\n",
    "axes[0, 0].set_title('Distribution of Days Past Due Categories')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Payment rate by DPD category\n",
    "payment_rate_by_dpd = df_analysis.groupby('DPD_Category')['Payment_Made_Last_30_Days'].mean() * 100\n",
    "axes[0, 1].bar(range(len(payment_rate_by_dpd)), payment_rate_by_dpd.values)\n",
    "axes[0, 1].set_xticks(range(len(payment_rate_by_dpd)))\n",
    "axes[0, 1].set_xticklabels(payment_rate_by_dpd.index, rotation=45)\n",
    "axes[0, 1].set_title('Recent Payment Rate by DPD Category')\n",
    "axes[0, 1].set_ylabel('Payment Rate (%)')\n",
    "\n",
    "# Outcome distribution by DPD category\n",
    "dpd_outcome.plot(kind='bar', ax=axes[1, 0], stacked=True)\n",
    "axes[1, 0].set_title('Outcome Distribution by DPD Category')\n",
    "axes[1, 0].set_ylabel('Percentage')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average outstanding balance by DPD category\n",
    "avg_balance_by_dpd = df_analysis.groupby('DPD_Category')['Outstanding_Balance'].mean()\n",
    "axes[1, 1].bar(range(len(avg_balance_by_dpd)), avg_balance_by_dpd.values)\n",
    "axes[1, 1].set_xticks(range(len(avg_balance_by_dpd)))\n",
    "axes[1, 1].set_xticklabels(avg_balance_by_dpd.index, rotation=45)\n",
    "axes[1, 1].set_title('Average Outstanding Balance by DPD Category')\n",
    "axes[1, 1].set_ylabel('Outstanding Balance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key insights\n",
    "insights = []\n",
    "\n",
    "# Target distribution insight\n",
    "paid_rate = (df['Outcome'] == 'Paid').mean() * 100\n",
    "insights.append(f\"Overall payment rate: {paid_rate:.1f}%\")\n",
    "\n",
    "# Best performing segments\n",
    "best_occupation = df.groupby('Occupation')['Outcome'].apply(lambda x: (x == 'Paid').mean()).idxmax()\n",
    "best_occupation_rate = df.groupby('Occupation')['Outcome'].apply(lambda x: (x == 'Paid').mean()).max() * 100\n",
    "insights.append(f\"Best performing occupation: {best_occupation} ({best_occupation_rate:.1f}% payment rate)\")\n",
    "\n",
    "best_region = df.groupby('Region')['Outcome'].apply(lambda x: (x == 'Paid').mean()).idxmax()\n",
    "best_region_rate = df.groupby('Region')['Outcome'].apply(lambda x: (x == 'Paid').mean()).max() * 100\n",
    "insights.append(f\"Best performing region: {best_region} ({best_region_rate:.1f}% payment rate)\")\n",
    "\n",
    "# Communication insights\n",
    "best_channel = df.groupby('Last_Contact_Channel')['Outcome'].apply(lambda x: (x == 'Paid').mean()).idxmax()\n",
    "best_channel_rate = df.groupby('Last_Contact_Channel')['Outcome'].apply(lambda x: (x == 'Paid').mean()).max() * 100\n",
    "insights.append(f\"Most effective contact channel: {best_channel} ({best_channel_rate:.1f}% payment rate)\")\n",
    "\n",
    "# Risk insights\n",
    "high_risk_payment_rate = df_analysis[df_analysis['Risk_Segment'] == 'High Risk']['Outcome'].apply(lambda x: x == 'Paid').mean() * 100\n",
    "low_risk_payment_rate = df_analysis[df_analysis['Risk_Segment'] == 'Low Risk']['Outcome'].apply(lambda x: x == 'Paid').mean() * 100\n",
    "insights.append(f\"Payment rate difference: Low Risk ({low_risk_payment_rate:.1f}%) vs High Risk ({high_risk_payment_rate:.1f}%)\")\n",
    "\n",
    "# Financial insights\n",
    "avg_debt_ratio_paid = df_analysis[df_analysis['Outcome'] == 'Paid']['Debt_to_Income_Ratio'].mean()\n",
    "avg_debt_ratio_not_paid = df_analysis[df_analysis['Outcome'] == 'Not Paid']['Debt_to_Income_Ratio'].mean()\n",
    "insights.append(f\"Debt-to-income ratio: Paid customers ({avg_debt_ratio_paid:.2f}) vs Non-paid ({avg_debt_ratio_not_paid:.2f})\")\n",
    "\n",
    "print(\"=== KEY INSIGHTS ===\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "# Business recommendations\n",
    "recommendations = [\n",
    "    f\"Focus collection efforts on {best_channel} channel for higher success rates\",\n",
    "    f\"Prioritize {best_occupation} customers as they show highest payment propensity\",\n",
    "    \"Implement early intervention for customers with debt-to-income ratio > 0.5\",\n",
    "    \"Develop specialized strategies for high-risk segments (DPD > 90 days)\",\n",
    "    \"Leverage response rate as a key predictor - customers with <20% response rate need different approach\",\n",
    "    f\"Expand operations in {best_region} region due to higher collection success\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== BUSINESS RECOMMENDATIONS ===\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "from data.data_preprocessor import DataQualityChecker\n",
    "\n",
    "quality_checker = DataQualityChecker()\n",
    "quality_report = quality_checker.assess_data_quality(df)\n",
    "\n",
    "print(quality_checker.generate_quality_report())\n",
    "\n",
    "# Visualize data quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Missing data visualization\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "if len(missing_data) > 0:\n",
    "    axes[0, 0].bar(range(len(missing_data)), missing_data.values)\n",
    "    axes[0, 0].set_xticks(range(len(missing_data)))\n",
    "    axes[0, 0].set_xticklabels(missing_data.index, rotation=45)\n",
    "    axes[0, 0].set_title('Missing Data by Column')\n",
    "    axes[0, 0].set_ylabel('Missing Count')\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No Missing Data', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "    axes[0, 0].set_title('Missing Data by Column')\n",
    "\n",
    "# Duplicate analysis\n",
    "duplicate_count = df.duplicated().sum()\n",
    "axes[0, 1].pie([len(df) - duplicate_count, duplicate_count], \n",
    "               labels=['Unique', 'Duplicates'], autopct='%1.1f%%')\n",
    "axes[0, 1].set_title(f'Duplicate Records ({duplicate_count} duplicates)')\n",
    "\n",
    "# Cardinality analysis\n",
    "cardinality = df.nunique().sort_values(ascending=False)\n",
    "axes[1, 0].bar(range(len(cardinality)), cardinality.values)\n",
    "axes[1, 0].set_xticks(range(len(cardinality)))\n",
    "axes[1, 0].set_xticklabels(cardinality.index, rotation=45)\n",
    "axes[1, 0].set_title('Feature Cardinality')\n",
    "axes[1, 0].set_ylabel('Unique Values')\n",
    "\n",
    "# Data types distribution\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "axes[1, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Data Types Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall Data Quality Score: {quality_report['quality_score']:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive EDA has revealed several key patterns in the debt collection data:\n",
    "\n",
    "1. **Target Distribution**: The dataset shows realistic class imbalance typical in debt collection scenarios\n",
    "2. **Demographic Patterns**: Clear differences in payment behavior across age groups, occupations, and regions\n",
    "3. **Financial Indicators**: Strong correlation between credit scores, debt-to-income ratios, and payment outcomes\n",
    "4. **Behavioral Insights**: Communication patterns and response rates are strong predictors of payment behavior\n",
    "5. **Risk Segmentation**: Clear risk tiers can be established based on multiple factors\n",
    "6. **Data Quality**: High-quality dataset with minimal missing data and realistic distributions\n",
    "\n",
    "These insights will inform our feature engineering and model development strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}