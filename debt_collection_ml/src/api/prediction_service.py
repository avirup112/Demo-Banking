from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, BackgroundTasks\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport json\nimport logging\nfrom datetime import datetime\nimport os\nfrom pathlib import Path\nimport io\nimport sys\n\n# Add src to path\nsys.path.append('src')\n\nfrom data.data_preprocessor import AdvancedDataPreprocessor\nfrom features.enhanced_feature_engineering import EnhancedFeatureEngineer\nfrom monitoring.drift_detection import DataDriftDetector\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"Debt Collection ML Prediction Service\",\n    description=\"Enhanced ML service for debt collection predictions with drift monitoring\",\n    version=\"2.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configure appropriately for production\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Security\nsecurity = HTTPBearer()\n\n# Pydantic models for API\nclass CustomerData(BaseModel):\n    \"\"\"Individual customer data for prediction\"\"\"\n    age: float = Field(..., description=\"Customer age\")\n    income: float = Field(..., description=\"Annual income\")\n    loan_amount: float = Field(..., description=\"Original loan amount\")\n    outstanding_balance: float = Field(..., description=\"Current outstanding balance\")\n    days_past_due: int = Field(..., description=\"Days past due\")\n    credit_score: float = Field(..., description=\"Credit score\")\n    employment_status: str = Field(..., description=\"Employment status\")\n    loan_purpose: str = Field(..., description=\"Purpose of loan\")\n    previous_defaults: int = Field(0, description=\"Number of previous defaults\")\n    contact_attempts: int = Field(0, description=\"Number of contact attempts\")\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"age\": 35.0,\n                \"income\": 50000.0,\n                \"loan_amount\": 15000.0,\n                \"outstanding_balance\": 8000.0,\n                \"days_past_due\": 45,\n                \"credit_score\": 650.0,\n                \"employment_status\": \"Employed\",\n                \"loan_purpose\": \"Personal\",\n                \"previous_defaults\": 0,\n                \"contact_attempts\": 3\n            }\n        }\n\nclass BatchPredictionRequest(BaseModel):\n    \"\"\"Batch prediction request\"\"\"\n    customers: List[CustomerData]\n    include_explanations: bool = Field(False, description=\"Include SHAP explanations\")\n    detect_drift: bool = Field(True, description=\"Perform drift detection\")\n\nclass PredictionResponse(BaseModel):\n    \"\"\"Prediction response\"\"\"\n    customer_id: Optional[str] = None\n    prediction: str = Field(..., description=\"Predicted outcome\")\n    probability: Dict[str, float] = Field(..., description=\"Class probabilities\")\n    confidence_score: float = Field(..., description=\"Prediction confidence\")\n    risk_category: str = Field(..., description=\"Risk category\")\n    explanation: Optional[Dict[str, Any]] = None\n    timestamp: str = Field(..., description=\"Prediction timestamp\")\n\nclass BatchPredictionResponse(BaseModel):\n    \"\"\"Batch prediction response\"\"\"\n    predictions: List[PredictionResponse]\n    batch_summary: Dict[str, Any]\n    drift_analysis: Optional[Dict[str, Any]] = None\n    processing_time: float\n    timestamp: str\n\nclass HealthResponse(BaseModel):\n    \"\"\"Health check response\"\"\"\n    status: str\n    model_loaded: bool\n    model_version: str\n    uptime: str\n    timestamp: str\n\n# Global variables for loaded models and components\nmodel = None\npreprocessor = None\nfeature_engineer = None\ndrift_detector = None\nmodel_metadata = {}\nreference_data = None\n\n# Model loading functions\ndef load_model_artifacts():\n    \"\"\"Load model and preprocessing artifacts\"\"\"\n    global model, preprocessor, feature_engineer, model_metadata, reference_data\n    \n    try:\n        # Load the best model (you can make this configurable)\n        model_files = list(Path('models/trained').glob('*_optimized.joblib'))\n        if not model_files:\n            model_files = list(Path('models/trained').glob('*.joblib'))\n        \n        if model_files:\n            # Load the most recent model\n            latest_model_file = max(model_files, key=os.path.getctime)\n            model = joblib.load(latest_model_file)\n            model_metadata['model_file'] = str(latest_model_file)\n            model_metadata['model_type'] = type(model).__name__\n            logger.info(f\"Model loaded: {latest_model_file}\")\n        else:\n            raise FileNotFoundError(\"No trained models found\")\n        \n        # Load preprocessor\n        preprocessor_path = 'models/artifacts/preprocessor_enhanced.joblib'\n        if os.path.exists(preprocessor_path):\n            preprocessor = joblib.load(preprocessor_path)\n            logger.info(\"Enhanced preprocessor loaded\")\n        else:\n            # Fallback to regular preprocessor\n            preprocessor_path = 'models/artifacts/preprocessor.joblib'\n            if os.path.exists(preprocessor_path):\n                preprocessor = joblib.load(preprocessor_path)\n                logger.info(\"Regular preprocessor loaded\")\n            else:\n                raise FileNotFoundError(\"No preprocessor found\")\n        \n        # Load feature engineer\n        feature_engineer_path = 'models/artifacts/feature_engineer_enhanced.joblib'\n        if os.path.exists(feature_engineer_path):\n            feature_engineer = joblib.load(feature_engineer_path)\n            logger.info(\"Enhanced feature engineer loaded\")\n        else:\n            # Fallback to regular feature engineer\n            feature_engineer_path = 'models/artifacts/feature_engineer.joblib'\n            if os.path.exists(feature_engineer_path):\n                feature_engineer = joblib.load(feature_engineer_path)\n                logger.info(\"Regular feature engineer loaded\")\n        \n        # Load reference data for drift detection\n        reference_data_path = 'data/processed/X_processed_enhanced.npy'\n        if os.path.exists(reference_data_path):\n            reference_data = np.load(reference_data_path)\n            logger.info(\"Reference data loaded for drift detection\")\n        \n        model_metadata['loaded_at'] = datetime.now().isoformat()\n        model_metadata['version'] = '2.0.0'\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"Failed to load model artifacts: {e}\")\n        return False\n\ndef initialize_drift_detector():\n    \"\"\"Initialize drift detector with reference data\"\"\"\n    global drift_detector, reference_data, preprocessor\n    \n    if reference_data is not None and preprocessor is not None:\n        try:\n            # Convert reference data to DataFrame\n            ref_df = pd.DataFrame(reference_data, columns=preprocessor.feature_names)\n            \n            # Initialize drift detector\n            drift_detector = DataDriftDetector(\n                reference_data=ref_df,\n                feature_columns=preprocessor.feature_names[:50],  # Use first 50 features\n                drift_threshold=0.05\n            )\n            \n            logger.info(\"Drift detector initialized\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize drift detector: {e}\")\n            return False\n    \n    return False\n\n# Authentication dependency\ndef get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Simple token-based authentication (implement proper auth in production)\"\"\"\n    # In production, validate the token properly\n    if credentials.credentials != \"your-secret-token\":\n        raise HTTPException(status_code=401, detail=\"Invalid authentication credentials\")\n    return {\"user_id\": \"api_user\"}\n\n# Startup event\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize the service on startup\"\"\"\n    logger.info(\"Starting Debt Collection ML Prediction Service...\")\n    \n    # Load model artifacts\n    if not load_model_artifacts():\n        logger.error(\"Failed to load model artifacts\")\n        raise RuntimeError(\"Model loading failed\")\n    \n    # Initialize drift detector\n    initialize_drift_detector()\n    \n    logger.info(\"Service initialized successfully\")\n\n# API endpoints\n@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return HealthResponse(\n        status=\"healthy\" if model is not None else \"unhealthy\",\n        model_loaded=model is not None,\n        model_version=model_metadata.get('version', 'unknown'),\n        uptime=\"N/A\",  # Implement uptime tracking if needed\n        timestamp=datetime.now().isoformat()\n    )\n\n@app.get(\"/model/info\")\nasync def get_model_info():\n    \"\"\"Get model information\"\"\"\n    if model is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    \n    return {\n        \"model_metadata\": model_metadata,\n        \"model_type\": type(model).__name__,\n        \"feature_count\": len(preprocessor.feature_names) if preprocessor else 0,\n        \"classes\": getattr(model, 'classes_', []).tolist() if hasattr(model, 'classes_') else []\n    }\n\ndef preprocess_customer_data(customer_data: CustomerData) -> pd.DataFrame:\n    \"\"\"Convert customer data to DataFrame and preprocess\"\"\"\n    \n    # Convert to DataFrame\n    data_dict = customer_data.dict()\n    df = pd.DataFrame([data_dict])\n    \n    # Add outcome column (dummy value for preprocessing)\n    df['Outcome'] = 'Not Paid'  # This will be ignored during prediction\n    \n    return df\n\ndef calculate_confidence_score(probabilities: np.ndarray) -> float:\n    \"\"\"Calculate confidence score from prediction probabilities\"\"\"\n    max_prob = np.max(probabilities)\n    # Confidence is the difference between highest and second highest probability\n    sorted_probs = np.sort(probabilities)[::-1]\n    if len(sorted_probs) > 1:\n        confidence = sorted_probs[0] - sorted_probs[1]\n    else:\n        confidence = max_prob\n    return float(confidence)\n\ndef determine_risk_category(prediction: str, confidence: float) -> str:\n    \"\"\"Determine risk category based on prediction and confidence\"\"\"\n    if prediction == 'Not Paid':\n        if confidence > 0.7:\n            return 'High Risk'\n        elif confidence > 0.4:\n            return 'Medium Risk'\n        else:\n            return 'Low-Medium Risk'\n    elif prediction == 'Partially Paid':\n        return 'Medium Risk'\n    else:  # Paid\n        if confidence > 0.7:\n            return 'Low Risk'\n        else:\n            return 'Low-Medium Risk'\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_single(\n    customer_data: CustomerData,\n    include_explanation: bool = False,\n    user: dict = Depends(get_current_user)\n):\n    \"\"\"Make prediction for a single customer\"\"\"\n    \n    if model is None or preprocessor is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    \n    try:\n        # Preprocess data\n        df = preprocess_customer_data(customer_data)\n        \n        # Apply preprocessing\n        X_processed, _ = preprocessor.transform(df, target_column='Outcome')\n        \n        # Apply feature engineering if available\n        if feature_engineer is not None:\n            feature_df = pd.DataFrame(X_processed, columns=preprocessor.feature_names)\n            X_engineered = feature_engineer.transform(feature_df)\n        else:\n            X_engineered = X_processed\n        \n        # Make prediction\n        prediction = model.predict(X_engineered)[0]\n        probabilities = model.predict_proba(X_engineered)[0]\n        \n        # Calculate confidence\n        confidence = calculate_confidence_score(probabilities)\n        \n        # Determine risk category\n        risk_category = determine_risk_category(prediction, confidence)\n        \n        # Create probability dictionary\n        classes = getattr(model, 'classes_', ['Not Paid', 'Paid', 'Partially Paid'])\n        prob_dict = {str(cls): float(prob) for cls, prob in zip(classes, probabilities)}\n        \n        # Generate explanation if requested\n        explanation = None\n        if include_explanation:\n            try:\n                import shap\n                # This is a simplified SHAP explanation - implement properly in production\n                explanation = {\"message\": \"SHAP explanations not implemented yet\"}\n            except ImportError:\n                explanation = {\"message\": \"SHAP not available\"}\n        \n        return PredictionResponse(\n            prediction=str(prediction),\n            probability=prob_dict,\n            confidence_score=confidence,\n            risk_category=risk_category,\n            explanation=explanation,\n            timestamp=datetime.now().isoformat()\n        )\n        \n    except Exception as e:\n        logger.error(f\"Prediction failed: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n\n@app.post(\"/predict/batch\", response_model=BatchPredictionResponse)\nasync def predict_batch(\n    request: BatchPredictionRequest,\n    background_tasks: BackgroundTasks,\n    user: dict = Depends(get_current_user)\n):\n    \"\"\"Make predictions for multiple customers\"\"\"\n    \n    if model is None or preprocessor is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    \n    start_time = datetime.now()\n    \n    try:\n        predictions = []\n        \n        # Process each customer\n        for i, customer_data in enumerate(request.customers):\n            try:\n                # Preprocess data\n                df = preprocess_customer_data(customer_data)\n                \n                # Apply preprocessing\n                X_processed, _ = preprocessor.transform(df, target_column='Outcome')\n                \n                # Apply feature engineering if available\n                if feature_engineer is not None:\n                    feature_df = pd.DataFrame(X_processed, columns=preprocessor.feature_names)\n                    X_engineered = feature_engineer.transform(feature_df)\n                else:\n                    X_engineered = X_processed\n                \n                # Make prediction\n                prediction = model.predict(X_engineered)[0]\n                probabilities = model.predict_proba(X_engineered)[0]\n                \n                # Calculate confidence\n                confidence = calculate_confidence_score(probabilities)\n                \n                # Determine risk category\n                risk_category = determine_risk_category(prediction, confidence)\n                \n                # Create probability dictionary\n                classes = getattr(model, 'classes_', ['Not Paid', 'Paid', 'Partially Paid'])\n                prob_dict = {str(cls): float(prob) for cls, prob in zip(classes, probabilities)}\n                \n                # Generate explanation if requested\n                explanation = None\n                if request.include_explanations:\n                    explanation = {\"message\": \"SHAP explanations not implemented yet\"}\n                \n                predictions.append(PredictionResponse(\n                    customer_id=f\"customer_{i+1}\",\n                    prediction=str(prediction),\n                    probability=prob_dict,\n                    confidence_score=confidence,\n                    risk_category=risk_category,\n                    explanation=explanation,\n                    timestamp=datetime.now().isoformat()\n                ))\n                \n            except Exception as e:\n                logger.error(f\"Failed to process customer {i+1}: {e}\")\n                # Add error prediction\n                predictions.append(PredictionResponse(\n                    customer_id=f\"customer_{i+1}\",\n                    prediction=\"Error\",\n                    probability={\"Error\": 1.0},\n                    confidence_score=0.0,\n                    risk_category=\"Unknown\",\n                    explanation={\"error\": str(e)},\n                    timestamp=datetime.now().isoformat()\n                ))\n        \n        # Create batch summary\n        successful_predictions = [p for p in predictions if p.prediction != \"Error\"]\n        batch_summary = {\n            \"total_customers\": len(request.customers),\n            \"successful_predictions\": len(successful_predictions),\n            \"failed_predictions\": len(predictions) - len(successful_predictions),\n            \"risk_distribution\": {}\n        }\n        \n        # Calculate risk distribution\n        if successful_predictions:\n            risk_counts = {}\n            for pred in successful_predictions:\n                risk_counts[pred.risk_category] = risk_counts.get(pred.risk_category, 0) + 1\n            batch_summary[\"risk_distribution\"] = risk_counts\n        \n        # Perform drift detection if requested\n        drift_analysis = None\n        if request.detect_drift and drift_detector is not None:\n            try:\n                # Convert batch data to DataFrame for drift detection\n                batch_df = pd.DataFrame([customer.dict() for customer in request.customers])\n                # Add dummy outcome column\n                batch_df['Outcome'] = 'Not Paid'\n                \n                # Preprocess for drift detection\n                X_batch, _ = preprocessor.transform(batch_df, target_column='Outcome')\n                batch_processed_df = pd.DataFrame(X_batch, columns=preprocessor.feature_names)\n                \n                # Run drift analysis in background\n                background_tasks.add_task(\n                    run_drift_analysis_background, \n                    batch_processed_df\n                )\n                \n                drift_analysis = {\"message\": \"Drift analysis running in background\"}\n                \n            except Exception as e:\n                logger.error(f\"Drift detection failed: {e}\")\n                drift_analysis = {\"error\": str(e)}\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        return BatchPredictionResponse(\n            predictions=predictions,\n            batch_summary=batch_summary,\n            drift_analysis=drift_analysis,\n            processing_time=processing_time,\n            timestamp=datetime.now().isoformat()\n        )\n        \n    except Exception as e:\n        logger.error(f\"Batch prediction failed: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Batch prediction failed: {str(e)}\")\n\nasync def run_drift_analysis_background(batch_data: pd.DataFrame):\n    \"\"\"Run drift analysis in background\"\"\"\n    try:\n        if drift_detector is not None:\n            results = drift_detector.comprehensive_drift_analysis(batch_data)\n            \n            # Save results\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            drift_detector.save_drift_report(\n                results, \n                f'reports/drift_analysis_{timestamp}.json'\n            )\n            \n            logger.info(f\"Drift analysis completed. Drift detected: {results['summary']['overall_drift_detected']}\")\n    except Exception as e:\n        logger.error(f\"Background drift analysis failed: {e}\")\n\n@app.post(\"/upload/csv\")\nasync def upload_csv_for_prediction(\n    file: UploadFile = File(...),\n    user: dict = Depends(get_current_user)\n):\n    \"\"\"Upload CSV file for batch prediction\"\"\"\n    \n    if not file.filename.endswith('.csv'):\n        raise HTTPException(status_code=400, detail=\"File must be a CSV\")\n    \n    try:\n        # Read CSV\n        contents = await file.read()\n        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))\n        \n        # Convert to CustomerData objects\n        customers = []\n        for _, row in df.iterrows():\n            try:\n                customer = CustomerData(**row.to_dict())\n                customers.append(customer)\n            except Exception as e:\n                logger.warning(f\"Skipping invalid row: {e}\")\n        \n        if not customers:\n            raise HTTPException(status_code=400, detail=\"No valid customer data found in CSV\")\n        \n        # Create batch request\n        batch_request = BatchPredictionRequest(\n            customers=customers,\n            include_explanations=False,\n            detect_drift=True\n        )\n        \n        # Process batch\n        return await predict_batch(batch_request, BackgroundTasks(), user)\n        \n    except Exception as e:\n        logger.error(f\"CSV upload failed: {e}\")\n        raise HTTPException(status_code=500, detail=f\"CSV processing failed: {str(e)}\")\n\n@app.get(\"/drift/status\")\nasync def get_drift_status(user: dict = Depends(get_current_user)):\n    \"\"\"Get current drift detection status\"\"\"\n    \n    if drift_detector is None:\n        return {\"status\": \"Drift detection not available\"}\n    \n    try:\n        # Get recent drift trends\n        trends = drift_detector.get_drift_trends(days=7)\n        \n        return {\n            \"drift_detector_active\": True,\n            \"recent_trends\": trends,\n            \"last_analysis\": drift_detector.drift_history[-1] if drift_detector.drift_history else None\n        }\n        \n    except Exception as e:\n        logger.error(f\"Failed to get drift status: {e}\")\n        return {\"error\": str(e)}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    # Run the service\n    uvicorn.run(\n        \"prediction_service:app\",\n        host=\"0.0.0.0\",\n        port=8000,\n        reload=True,\n        log_level=\"info\"\n    )