import mlflow\nimport mlflow.sklearn\nimport mlflow.xgboost\nimport mlflow.lightgbm\nfrom mlflow.tracking import MlflowClient\nfrom mlflow.models.signature import infer_signature\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom datetime import datetime\nimport os\nimport json\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\nclass MLflowManager:\n    \"\"\"MLflow integration for experiment tracking and model registry\"\"\"\n    \n    def __init__(self, \n                 experiment_name: str = \"debt_collection_ml\",\n                 tracking_uri: Optional[str] = None,\n                 registry_uri: Optional[str] = None):\n        \n        self.experiment_name = experiment_name\n        self.tracking_uri = tracking_uri or \"./mlruns\"\n        self.registry_uri = registry_uri or self.tracking_uri\n        \n        # Set up MLflow\n        mlflow.set_tracking_uri(self.tracking_uri)\n        \n        # Create experiment if it doesn't exist\n        try:\n            self.experiment_id = mlflow.create_experiment(experiment_name)\n        except mlflow.exceptions.MlflowException:\n            self.experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n        \n        mlflow.set_experiment(experiment_name)\n        \n        # Initialize client\n        self.client = MlflowClient()\n        \n        self.logger = logging.getLogger(__name__)\n        self.logger.info(f\"MLflow initialized with experiment: {experiment_name}\")\n    \n    def start_run(self, run_name: Optional[str] = None, tags: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"Start a new MLflow run\"\"\"\n        \n        run_tags = tags or {}\n        run_tags.update({\n            \"mlflow.runName\": run_name or f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n            \"project\": \"debt_collection_ml\",\n            \"timestamp\": datetime.now().isoformat()\n        })\n        \n        run = mlflow.start_run(tags=run_tags)\n        self.logger.info(f\"Started MLflow run: {run.info.run_id}\")\n        \n        return run.info.run_id\n    \n    def log_parameters(self, params: Dict[str, Any]):\n        \"\"\"Log parameters to MLflow\"\"\"\n        \n        for key, value in params.items():\n            try:\n                # MLflow has limitations on parameter values\n                if isinstance(value, (dict, list)):\n                    mlflow.log_param(key, str(value)[:250])  # Truncate if too long\n                else:\n                    mlflow.log_param(key, value)\n            except Exception as e:\n                self.logger.warning(f\"Failed to log parameter {key}: {e}\")\n    \n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):\n        \"\"\"Log metrics to MLflow\"\"\"\n        \n        for key, value in metrics.items():\n            try:\n                mlflow.log_metric(key, float(value), step=step)\n            except Exception as e:\n                self.logger.warning(f\"Failed to log metric {key}: {e}\")\n    \n    def log_model(self, \n                  model: Any, \n                  model_name: str,\n                  X_sample: np.ndarray,\n                  model_type: str = \"sklearn\",\n                  artifacts: Optional[Dict[str, str]] = None,\n                  conda_env: Optional[str] = None) -> str:\n        \"\"\"Log model to MLflow with signature and artifacts\"\"\"\n        \n        try:\n            # Infer model signature\n            signature = infer_signature(X_sample, model.predict(X_sample))\n            \n            # Log model based on type\n            if model_type == \"sklearn\":\n                model_info = mlflow.sklearn.log_model(\n                    sk_model=model,\n                    artifact_path=model_name,\n                    signature=signature,\n                    conda_env=conda_env\n                )\n            elif model_type == \"xgboost\":\n                model_info = mlflow.xgboost.log_model(\n                    xgb_model=model,\n                    artifact_path=model_name,\n                    signature=signature,\n                    conda_env=conda_env\n                )\n            elif model_type == \"lightgbm\":\n                model_info = mlflow.lightgbm.log_model(\n                    lgb_model=model,\n                    artifact_path=model_name,\n                    signature=signature,\n                    conda_env=conda_env\n                )\n            else:\n                # Generic model logging\n                model_info = mlflow.sklearn.log_model(\n                    sk_model=model,\n                    artifact_path=model_name,\n                    signature=signature,\n                    conda_env=conda_env\n                )\n            \n            # Log additional artifacts\n            if artifacts:\n                for artifact_name, artifact_path in artifacts.items():\n                    if os.path.exists(artifact_path):\n                        mlflow.log_artifact(artifact_path, artifact_name)\n            \n            self.logger.info(f\"Model {model_name} logged successfully\")\n            return model_info.model_uri\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to log model {model_name}: {e}\")\n            return None\n    \n    def log_feature_importance(self, model: Any, feature_names: List[str]):\n        \"\"\"Log feature importance as artifacts\"\"\"\n        \n        try:\n            if hasattr(model, 'feature_importances_'):\n                importance_df = pd.DataFrame({\n                    'feature': feature_names[:len(model.feature_importances_)],\n                    'importance': model.feature_importances_\n                }).sort_values('importance', ascending=False)\n                \n                # Save as CSV\n                importance_path = \"feature_importance.csv\"\n                importance_df.to_csv(importance_path, index=False)\n                mlflow.log_artifact(importance_path)\n                \n                # Log top 10 as metrics\n                for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n                    mlflow.log_metric(f\"feature_importance_{i+1}_{row['feature']}\", row['importance'])\n                \n                # Clean up\n                os.remove(importance_path)\n                \n                self.logger.info(\"Feature importance logged\")\n                \n        except Exception as e:\n            self.logger.warning(f\"Failed to log feature importance: {e}\")\n    \n    def log_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, class_names: List[str]):\n        \"\"\"Log confusion matrix as artifact\"\"\"\n        \n        try:\n            from sklearn.metrics import confusion_matrix\n            import matplotlib.pyplot as plt\n            import seaborn as sns\n            \n            cm = confusion_matrix(y_true, y_pred)\n            \n            # Create heatmap\n            plt.figure(figsize=(8, 6))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                       xticklabels=class_names, yticklabels=class_names)\n            plt.title('Confusion Matrix')\n            plt.ylabel('True Label')\n            plt.xlabel('Predicted Label')\n            \n            # Save and log\n            cm_path = \"confusion_matrix.png\"\n            plt.savefig(cm_path, dpi=150, bbox_inches='tight')\n            mlflow.log_artifact(cm_path)\n            \n            # Clean up\n            plt.close()\n            os.remove(cm_path)\n            \n            self.logger.info(\"Confusion matrix logged\")\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to log confusion matrix: {e}\")\n    \n    def log_training_data_info(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Log training data information\"\"\"\n        \n        try:\n            # Data shape and basic stats\n            mlflow.log_param(\"n_samples\", X.shape[0])\n            mlflow.log_param(\"n_features\", X.shape[1])\n            \n            # Class distribution\n            unique, counts = np.unique(y, return_counts=True)\n            class_dist = dict(zip(unique, counts))\n            \n            for class_label, count in class_dist.items():\n                mlflow.log_metric(f\"class_{class_label}_count\", count)\n                mlflow.log_metric(f\"class_{class_label}_percentage\", count / len(y) * 100)\n            \n            # Data quality metrics\n            if hasattr(X, 'isnull'):\n                missing_percentage = X.isnull().sum().sum() / (X.shape[0] * X.shape[1]) * 100\n                mlflow.log_metric(\"missing_data_percentage\", missing_percentage)\n            \n            self.logger.info(\"Training data info logged\")\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to log training data info: {e}\")\n    \n    def register_model(self, \n                      model_uri: str, \n                      model_name: str, \n                      stage: str = \"Staging\",\n                      description: Optional[str] = None,\n                      tags: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"Register model in MLflow Model Registry\"\"\"\n        \n        try:\n            # Register model\n            model_version = mlflow.register_model(\n                model_uri=model_uri,\n                name=model_name,\n                tags=tags\n            )\n            \n            # Update model version\n            if description:\n                self.client.update_model_version(\n                    name=model_name,\n                    version=model_version.version,\n                    description=description\n                )\n            \n            # Transition to stage\n            self.client.transition_model_version_stage(\n                name=model_name,\n                version=model_version.version,\n                stage=stage\n            )\n            \n            self.logger.info(f\"Model {model_name} v{model_version.version} registered in {stage}\")\n            return model_version.version\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to register model {model_name}: {e}\")\n            return None\n    \n    def get_latest_model_version(self, model_name: str, stage: str = \"Production\") -> Optional[str]:\n        \"\"\"Get latest model version from registry\"\"\"\n        \n        try:\n            latest_version = self.client.get_latest_versions(\n                name=model_name,\n                stages=[stage]\n            )\n            \n            if latest_version:\n                return latest_version[0].version\n            else:\n                return None\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to get latest model version: {e}\")\n            return None\n    \n    def load_model(self, model_name: str, version: Optional[str] = None, stage: Optional[str] = None) -> Any:\n        \"\"\"Load model from registry\"\"\"\n        \n        try:\n            if version:\n                model_uri = f\"models:/{model_name}/{version}\"\n            elif stage:\n                model_uri = f\"models:/{model_name}/{stage}\"\n            else:\n                model_uri = f\"models:/{model_name}/latest\"\n            \n            model = mlflow.sklearn.load_model(model_uri)\n            self.logger.info(f\"Model loaded from {model_uri}\")\n            return model\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load model: {e}\")\n            return None\n    \n    def compare_models(self, run_ids: List[str], metrics: List[str]) -> pd.DataFrame:\n        \"\"\"Compare multiple model runs\"\"\"\n        \n        try:\n            comparison_data = []\n            \n            for run_id in run_ids:\n                run = self.client.get_run(run_id)\n                \n                row_data = {'run_id': run_id}\n                row_data.update(run.data.params)\n                \n                for metric in metrics:\n                    if metric in run.data.metrics:\n                        row_data[metric] = run.data.metrics[metric]\n                    else:\n                        row_data[metric] = None\n                \n                comparison_data.append(row_data)\n            \n            comparison_df = pd.DataFrame(comparison_data)\n            self.logger.info(f\"Compared {len(run_ids)} model runs\")\n            return comparison_df\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to compare models: {e}\")\n            return pd.DataFrame()\n    \n    def log_hyperparameter_optimization(self, study, model_name: str):\n        \"\"\"Log Optuna study results to MLflow\"\"\"\n        \n        try:\n            # Log best parameters\n            best_params = study.best_params\n            for param, value in best_params.items():\n                mlflow.log_param(f\"best_{param}\", value)\n            \n            # Log best score\n            mlflow.log_metric(\"best_cv_score\", study.best_value)\n            \n            # Log optimization history\n            optimization_history = [trial.value for trial in study.trials if trial.value is not None]\n            \n            for i, score in enumerate(optimization_history):\n                mlflow.log_metric(\"optimization_score\", score, step=i)\n            \n            # Save study as artifact\n            study_path = f\"{model_name}_optuna_study.pkl\"\n            joblib.dump(study, study_path)\n            mlflow.log_artifact(study_path)\n            os.remove(study_path)\n            \n            self.logger.info(f\"Hyperparameter optimization results logged for {model_name}\")\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to log optimization results: {e}\")\n    \n    def end_run(self):\n        \"\"\"End current MLflow run\"\"\"\n        \n        try:\n            mlflow.end_run()\n            self.logger.info(\"MLflow run ended\")\n        except Exception as e:\n            self.logger.warning(f\"Failed to end run properly: {e}\")\n    \n    def create_model_deployment_config(self, \n                                     model_name: str, \n                                     version: str,\n                                     target_environment: str = \"staging\") -> Dict[str, Any]:\n        \"\"\"Create deployment configuration for model\"\"\"\n        \n        config = {\n            \"model_name\": model_name,\n            \"model_version\": version,\n            \"model_uri\": f\"models:/{model_name}/{version}\",\n            \"target_environment\": target_environment,\n            \"deployment_timestamp\": datetime.now().isoformat(),\n            \"deployment_config\": {\n                \"cpu_limit\": \"2\",\n                \"memory_limit\": \"4Gi\",\n                \"replicas\": 2 if target_environment == \"production\" else 1,\n                \"health_check_path\": \"/health\",\n                \"metrics_path\": \"/metrics\"\n            }\n        }\n        \n        return config\n    \n    def get_experiment_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of current experiment\"\"\"\n        \n        try:\n            experiment = self.client.get_experiment(self.experiment_id)\n            runs = self.client.search_runs(experiment_ids=[self.experiment_id])\n            \n            summary = {\n                \"experiment_name\": experiment.name,\n                \"experiment_id\": experiment.experiment_id,\n                \"total_runs\": len(runs),\n                \"active_runs\": len([r for r in runs if r.info.status == \"RUNNING\"]),\n                \"completed_runs\": len([r for r in runs if r.info.status == \"FINISHED\"]),\n                \"failed_runs\": len([r for r in runs if r.info.status == \"FAILED\"]),\n                \"creation_time\": experiment.creation_time,\n                \"last_update_time\": experiment.last_update_time\n            }\n            \n            # Get best run by F1 score\n            if runs:\n                best_run = max(runs, key=lambda r: r.data.metrics.get('f1_weighted', 0))\n                summary[\"best_run\"] = {\n                    \"run_id\": best_run.info.run_id,\n                    \"f1_score\": best_run.data.metrics.get('f1_weighted', 0),\n                    \"accuracy\": best_run.data.metrics.get('accuracy', 0)\n                }\n            \n            return summary\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get experiment summary: {e}\")\n            return {}\n\nclass MLflowModelTracker:\n    \"\"\"High-level interface for tracking ML experiments\"\"\"\n    \n    def __init__(self, experiment_name: str = \"debt_collection_ml\"):\n        self.mlflow_manager = MLflowManager(experiment_name)\n        self.current_run_id = None\n        self.logger = logging.getLogger(__name__)\n    \n    def track_training_session(self, \n                             models: Dict[str, Any],\n                             model_results: Dict[str, Dict[str, float]],\n                             training_config: Dict[str, Any],\n                             X_train: np.ndarray,\n                             y_train: np.ndarray,\n                             X_test: np.ndarray,\n                             y_test: np.ndarray,\n                             feature_names: Optional[List[str]] = None,\n                             optimization_studies: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Track complete training session\"\"\"\n        \n        # Start run\n        run_name = f\"training_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.current_run_id = self.mlflow_manager.start_run(\n            run_name=run_name,\n            tags={\"session_type\": \"complete_training\"}\n        )\n        \n        try:\n            # Log training configuration\n            self.mlflow_manager.log_parameters(training_config)\n            \n            # Log training data info\n            self.mlflow_manager.log_training_data_info(X_train, y_train)\n            \n            # Log each model\n            for model_name, model in models.items():\n                if model_name in model_results:\n                    # Log model metrics\n                    metrics = {f\"{model_name}_{k}\": v for k, v in model_results[model_name].items()}\n                    self.mlflow_manager.log_metrics(metrics)\n                    \n                    # Log model artifact\n                    model_uri = self.mlflow_manager.log_model(\n                        model=model,\n                        model_name=model_name,\n                        X_sample=X_test[:100],  # Sample for signature\n                        model_type=\"sklearn\"  # Default to sklearn\n                    )\n                    \n                    # Log feature importance if available\n                    if feature_names and hasattr(model, 'feature_importances_'):\n                        self.mlflow_manager.log_feature_importance(model, feature_names)\n                    \n                    # Log confusion matrix\n                    try:\n                        y_pred = model.predict(X_test)\n                        class_names = [f\"Class_{i}\" for i in np.unique(y_test)]\n                        self.mlflow_manager.log_confusion_matrix(y_test, y_pred, class_names)\n                    except Exception as e:\n                        self.logger.warning(f\"Failed to log confusion matrix for {model_name}: {e}\")\n            \n            # Log optimization studies if available\n            if optimization_studies:\n                for model_name, study in optimization_studies.items():\n                    self.mlflow_manager.log_hyperparameter_optimization(study, model_name)\n            \n            # Find and log best model\n            best_model_name = max(model_results.keys(), \n                                key=lambda k: model_results[k].get('f1_weighted', 0))\n            \n            self.mlflow_manager.log_metrics({\n                \"best_model_f1\": model_results[best_model_name]['f1_weighted'],\n                \"best_model_accuracy\": model_results[best_model_name]['accuracy']\n            })\n            \n            mlflow.set_tag(\"best_model\", best_model_name)\n            \n            self.logger.info(f\"Training session tracked successfully. Run ID: {self.current_run_id}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to track training session: {e}\")\n        \n        finally:\n            self.mlflow_manager.end_run()\n        \n        return self.current_run_id\n\nif __name__ == \"__main__\":\n    # Test MLflow integration\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, f1_score\n    \n    # Create sample data\n    np.random.seed(42)\n    X = np.random.randn(1000, 10)\n    y = np.random.randint(0, 3, 1000)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train a simple model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    # Test MLflow tracking\n    tracker = MLflowModelTracker(\"test_experiment\")\n    \n    models = {\"RandomForest\": model}\n    results = {\n        \"RandomForest\": {\n            \"accuracy\": accuracy_score(y_test, y_pred),\n            \"f1_weighted\": f1_score(y_test, y_pred, average='weighted')\n        }\n    }\n    \n    config = {\"n_estimators\": 100, \"random_state\": 42}\n    \n    run_id = tracker.track_training_session(\n        models=models,\n        model_results=results,\n        training_config=config,\n        X_train=X_train,\n        y_train=y_train,\n        X_test=X_test,\n        y_test=y_test\n    )\n    \n    print(f\"MLflow tracking test completed. Run ID: {run_id}\")