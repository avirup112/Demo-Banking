# ğŸ¦ Debt Collection ML System

A production-ready AI/ML system for debt collection optimization with complete **DVC pipeline orchestration**. Predicts repayment probability and provides actionable insights for collection strategies.

## ğŸ¯ Project Overview

This system addresses the complete debt collection lifecycle from assignment to closure:

- **ğŸ¯ Repayment Probability Prediction**: Advanced ML models (XGBoost, LightGBM, Random Forest)
- **ğŸ“Š Risk-Based Prioritization**: Intelligent customer segmentation and prioritization
- **ğŸ“ Contact Optimization**: AI-driven recommendations for optimal communication channels and timing
- **ğŸ” Explainable AI**: SHAP explanations for transparent model decisions
- **âš™ï¸ Complete MLOps**: DVC pipelines, monitoring, drift detection, and automated workflows
- **ğŸš€ Interactive Dashboard**: Real-time Streamlit dashboard with model insights

## ğŸ—ï¸ **Project Structure**

```
debt-collection-ml-system/
â”œâ”€â”€ ğŸš€ MAIN ENTRY POINTS
â”‚   â”œâ”€â”€ run_all.py                    # One-click ML pipeline + dashboard
â”‚   â”œâ”€â”€ run_complete_pipeline.py     # Complete ML workflow
â”‚   â”œâ”€â”€ run_enhanced_pipeline.py     # Enhanced ML pipeline
â”‚   â””â”€â”€ streamlit_dashboard.py       # Interactive dashboard
â”‚
â”œâ”€â”€ ğŸ“Š SOURCE CODE
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ data/                     # Data generation & preprocessing
â”‚       â”‚   â”œâ”€â”€ data_generator.py     # Synthetic data creation
â”‚       â”‚   â””â”€â”€ data_preprocessor.py  # Data cleaning & validation
â”‚       â”œâ”€â”€ features/                 # Feature engineering
â”‚       â”‚   â””â”€â”€ feature_engineering.py # Advanced feature creation
â”‚       â”œâ”€â”€ explainability/           # Model explanations
â”‚       â”‚   â””â”€â”€ shap_explainer.py     # SHAP-based explanations
â”‚       â”œâ”€â”€ optimization/             # Hyperparameter optimization
â”‚       â”‚   â”œâ”€â”€ simple_optimizer.py   # Fast grid search
â”‚       â”‚   â””â”€â”€ fast_grid_search.py   # Optimized parameter search
â”‚       â”œâ”€â”€ validation/               # Model validation
â”‚       â”‚   â””â”€â”€ model_validator.py    # Comprehensive model testing
â”‚       â”œâ”€â”€ recommendations/          # Business recommendations
â”‚       â”‚   â””â”€â”€ contact_optimizer.py  # Contact strategy optimization
â”‚       â”œâ”€â”€ testing/                  # A/B testing framework
â”‚       â”‚   â””â”€â”€ ab_testing.py         # Experiment management
â”‚       â”œâ”€â”€ monitoring/               # Model monitoring
â”‚       â”‚   â””â”€â”€ drift_detector.py     # Data drift detection
â”‚       â”œâ”€â”€ deployment/               # Production deployment
â”‚       â”‚   â””â”€â”€ deployment_manager.py # Model deployment utilities
â”‚       â””â”€â”€ utils/                    # Utilities
â”‚           â””â”€â”€ dagshub_integration.py # MLOps integration
â”‚
â”œâ”€â”€ âš™ï¸ CONFIGURATION & PIPELINE
â”‚   â”œâ”€â”€ dvc.yaml                      # DVC pipeline definition
â”‚   â”œâ”€â”€ params.yaml                   # Pipeline parameters
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                    # Container configuration
â”‚   â””â”€â”€ docker-compose.yml            # Multi-service setup
â”‚
â”œâ”€â”€ ğŸ“ DATA & ARTIFACTS (DVC-tracked)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/                      # Original datasets
â”‚   â”‚   â””â”€â”€ processed/                # Processed features
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ trained/                  # Trained model artifacts
â”‚   â”‚   â””â”€â”€ artifacts/                # Preprocessing artifacts
â”‚   â”œâ”€â”€ reports/                      # Generated reports
â”‚   â”œâ”€â”€ explanations/                 # SHAP plots and insights
â”‚   â”œâ”€â”€ ab_experiments/               # A/B test results
â”‚   â”œâ”€â”€ validation_results/           # Model validation reports
â”‚   â””â”€â”€ monitoring_results/           # Drift detection results
â”‚
â””â”€â”€ ğŸ“š DOCUMENTATION
    â”œâ”€â”€ README.md                     # This file
    â””â”€â”€ notebooks/                    # Analysis notebooks
        â””â”€â”€ 01_comprehensive_eda.ipynb # Exploratory data analysis
```

## ğŸš€ Quick Start

### ğŸ¯ **DVC Pipeline (Recommended - One Command)**

```bash
# Clone and setup
git clone https://github.com/avirup112/Demo-Banking.git
pip install -r ../requirements.txt

# Run complete pipeline with one command
dvc repro

# ğŸ‰ This automatically:
# âœ… Generates synthetic debt collection data (10,000 samples)
# âœ… Preprocesses and engineers features
# âœ… Trains multiple ML models (XGBoost, LightGBM, Random Forest)
# âœ… Optimizes hyperparameters with Optuna
# âœ… Generates SHAP explanations
# âœ… Creates comprehensive reports
# âœ… Launches interactive dashboard at http://localhost:8501
# âœ… Opens browser automatically
```

### âš¡ **Alternative Quick Runs**

```bash
# Option 1: Direct Python execution (fast)
python run_all.py --quick --samples 1000

# Option 2: Enhanced pipeline with custom settings
python run_enhanced_pipeline.py --samples 5000 --optimization-method optuna

# Option 3: Complete pipeline with dashboard
python run_complete_pipeline.py --samples 10000 --dashboard-timeout 600
```

### ğŸ“Š **DVC Pipeline Management**

```bash
# Check pipeline status
dvc status

# View pipeline structure
dvc dag

# Run specific stages only
dvc repro data_generation
dvc repro model_training

# View metrics and results
dvc metrics show
dvc plots show

# Push/pull data (if using remote storage)
dvc push
dvc pull
```

### ğŸ³ **Docker Deployment**

```bash
# Build and run with Docker Compose
docker-compose up --build

# Access services:
# - Dashboard: http://localhost:8501
# - API: http://localhost:8000  
# - MLflow UI: http://localhost:5000
# - Jupyter: http://localhost:8888
```

### ğŸ”§ **Development Setup**

```bash
# Create virtual environment
python -m venv myenv
source myenv/bin/activate  # Windows: myenv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run DVC pipeline
cd debt_collection_ml
dvc repro

# Or run components individually
python run_enhanced_pipeline.py
streamlit run streamlit_dashboard.py
```

## ğŸ“ˆ Features

### 1. Data Understanding & Preprocessing
- **Comprehensive EDA**: 10+ visualization types, statistical analysis
- **Advanced Preprocessing**: Multiple imputation strategies, outlier handling
- **Data Quality Assessment**: Automated quality scoring and reporting
- **Feature Engineering**: Domain-specific financial ratios, behavioral patterns

### 2. Model Architecture & Scalability
- **Multiple Algorithms**: XGBoost, LightGBM, Random Forest, Ensemble
- **Hyperparameter Optimization**: Optuna-based automated tuning
- **Class Imbalance Handling**: SMOTE, ADASYN, SMOTETomek
- **Modular Design**: Easily extensible architecture
- **Containerization**: Docker support for scalable deployment

### 3. Predictive Accuracy & Metrics
- **Comprehensive Evaluation**: ROC-AUC, F1, Precision, Recall
- **Business Metrics**: Recovery precision, collection recall, expected recovery rate
- **Cross-Validation**: Stratified K-fold with robust validation
- **Model Comparison**: Automated comparison across multiple algorithms

### 4. Explainability & Interpretability
- **SHAP Integration**: Global and local explanations
- **LIME Support**: Instance-level explanations
- **Feature Importance**: Multiple importance calculation methods
- **Business Explanations**: Domain-specific interpretation of model decisions

### 5. Production Readiness & MLOps
- **Model Registry**: DagsHub + Local SQLite-based model versioning
- **Data Versioning**: DVC for data and model artifact versioning
- **Pipeline Management**: DVC pipelines for reproducible workflows
- **Monitoring**: Data drift detection, performance monitoring
- **CI/CD Pipeline**: Automated testing and validation
- **Experiment Tracking**: DagsHub + MLflow integration
- **Health Checks**: Model and data validation

### 6. Innovation & Presentation
- **Interactive Dashboard**: Streamlit-based visualization
- **Web Scraping**: External data enrichment capabilities
- **REST API**: FastAPI-based model serving
- **Comprehensive Documentation**: Detailed guides and examples

## ğŸ”§ Usage Examples

### DVC Pipeline Management

```bash
# Initialize DVC pipeline
python scripts/dvc_pipeline.py init

# Run complete pipeline
python scripts/dvc_pipeline.py run

# Run specific stages
python scripts/dvc_pipeline.py run --stages data_generation model_training

# Show pipeline status
python scripts/dvc_pipeline.py status

# View metrics
python scripts/dvc_pipeline.py metrics

# Create experiment
python scripts/dvc_pipeline.py experiment run --name "experiment_1" --param training.n_trials=100

# Push data to DagsHub
python scripts/dvc_pipeline.py push
```

### Training Models

```python
from src.models.ml_model import DebtCollectionMLModel

# Initialize and train model
model = DebtCollectionMLModel(model_type='xgboost')
model.train(X_train, y_train, optimize=True)

# Evaluate model
results = model.evaluate(X_test, y_test)
print(f"Business F1 Score: {results['business_metrics']['business_f1']:.4f}")
```

### Making Predictions

```python
# Load trained model
model.load_model('models/trained/xgboost_model.joblib')

# Make predictions
predictions = model.predict(X_new)
probabilities = model.predict_proba(X_new)
```

### Model Explanations

```python
from src.models.explainability import ModelExplainer

explainer = ModelExplainer(model, X_train, feature_names)
explainer.explain_instance_shap(X_instance[0])
explainer.global_feature_importance_shap()
```

### Recommendations

```python
from src.models.recommendations import RecommendationEngine

recommender = RecommendationEngine()
recommender.train_channel_recommendation_model(df)

# Get recommendations
recommendation = recommender.get_comprehensive_recommendation(customer_data)
print(f"Recommended channel: {recommendation['channel_recommendation']['channel']}")
```

<!-- ## ğŸ“Š Model Performance

| Model | Accuracy | F1-Score | ROC-AUC | Business F1 | Recovery Precision |
|-------|----------|----------|---------|-------------|-------------------|
| XGBoost | 0.847 | 0.834 | 0.891 | 0.823 | 0.856 |
| LightGBM | 0.842 | 0.829 | 0.887 | 0.818 | 0.851 |
| Random Forest | 0.839 | 0.825 | 0.883 | 0.814 | 0.847 |
| Ensemble | 0.851 | 0.838 | 0.894 | 0.827 | 0.859 | -->

## ğŸ” Key Insights

1. **Payment Behavior Patterns**: Clear correlation between credit scores, response rates, and payment outcomes
2. **Channel Effectiveness**: WhatsApp and Email show higher engagement rates for younger demographics
3. **Risk Segmentation**: 3-tier risk model effectively separates high/medium/low probability customers
4. **Temporal Patterns**: Days past due is the strongest predictor, with 90+ days showing critical risk threshold

## ğŸ› ï¸ MLOps Features

### Data Version Control (DVC)
- **Data Versioning**: Track changes in datasets and model artifacts
- **Pipeline Management**: Reproducible ML pipelines with dependency tracking
- **Experiment Tracking**: Parameter and metric comparison across runs
- **Remote Storage**: Support for S3, GCS, Azure, SSH, and local storage

### Model Registry
- Version control for models with DagsHub integration
- Metadata tracking and model promotion workflows
- Automated model comparison and selection

### Monitoring
- Data drift detection using Evidently
- Performance degradation alerts
- Real-time metrics tracking with MLflow

### CI/CD Pipeline
- Automated data validation and quality checks
- Model testing and validation pipelines
- DVC-based reproducible deployments

### Quick DVC Commands
```bash
# Run full pipeline
dvc repro

# Check pipeline status
dvc status

# View pipeline DAG
dvc dag

# Compare experiments
dvc params diff
dvc metrics diff

# Push/pull data
dvc push
dvc pull
```

## ğŸ”„ **DVC Pipeline Architecture**

The entire system is orchestrated through a **DVC pipeline** that ensures reproducibility and version control:

```mermaid
graph TD
    A[ğŸ² data_generation] --> B[ğŸ§¹ data_preprocessing]
    B --> C[âš™ï¸ feature_engineering] 
    C --> D[ğŸ¤– complete_pipeline]
    
    A --> E[ğŸ“Š data/raw/debt_collection_data.csv]
    B --> F[ğŸ“ˆ data/processed/X_processed.npy]
    B --> G[ğŸ¯ data/processed/y_encoded.npy]
    C --> H[ğŸ”§ data/processed/X_engineered.npy]
    D --> I[ğŸ† models/trained/]
    D --> J[ğŸ“‹ reports/]
    D --> K[ğŸ¨ explanations/]
    D --> L[ğŸš€ Dashboard Launch]
```

### **Pipeline Stages Explained:**

| Stage | Description | Outputs | Duration |
|-------|-------------|---------|----------|
| **ğŸ² data_generation** | Creates synthetic debt collection dataset | `data/raw/debt_collection_data.csv` | ~30s |
| **ğŸ§¹ data_preprocessing** | Cleans, validates, and preprocesses data | `data/processed/X_processed.npy`, `y_encoded.npy` | ~45s |
| **âš™ï¸ feature_engineering** | Creates advanced financial and behavioral features | `data/processed/X_engineered.npy` | ~60s |
| **ğŸ¤– complete_pipeline** | Trains models, optimizes, explains, launches dashboard | `models/`, `reports/`, `explanations/`, Dashboard | ~5-10min |

(what was observe on first-run)

### **Run the Complete Pipeline:**

```bash
# Single command runs everything
dvc repro

# âœ… Total time: ~7-12 minutes
# âœ… Automatic dashboard launch
# âœ… All artifacts versioned and tracked
# âœ… Reproducible across environments
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‰ **Expected Results**

After running `dvc repro`, you'll have:

### **ğŸ“Š Model Performance:**
- **XGBoost**: F1-Score ~0.85, ROC-AUC ~0.91
- **LightGBM**: F1-Score ~0.83, ROC-AUC ~0.89  
- **Random Forest**: F1-Score ~0.81, ROC-AUC ~0.87
- **Business Metrics**: Recovery precision >80%, Collection recall >75%

### **ğŸ¨ Interactive Dashboard:**
- **Customer Risk Scoring**: Real-time probability predictions
- **Feature Explanations**: SHAP-based model interpretability
- **Business Recommendations**: Optimal contact strategies
- **Performance Monitoring**: Model metrics and data quality

### **ğŸ“‹ Comprehensive Reports:**
- **Model Comparison**: Detailed performance analysis
- **Feature Importance**: Business-relevant insights
- **Validation Results**: Robust model evaluation
- **A/B Testing**: Experiment tracking and results

### **ğŸ” Key Business Insights:**
- **Payment Behavior**: Credit score and days past due are strongest predictors
- **Channel Effectiveness**: WhatsApp and Email show highest engagement
- **Risk Segmentation**: Clear separation of high/medium/low risk customers
- **Temporal Patterns**: 90+ days past due represents critical threshold

---

## ğŸš€ **Get Started Now:**

```bash
git clone https://github.com/YOUR_USERNAME/debt-collection-ml-system.git
cd debt-collection-ml-system/debt_collection_ml
pip install -r ../requirements.txt
dvc repro
```

**ğŸ¯ In ~10 minutes, you'll have a complete ML system with interactive dashboard!**

---

**Note**: This system uses synthetic data for demonstration purposes. In production, ensure compliance with data privacy regulations and ethical AI practices.
## ï¿½ **What You Get After `dvc repro`**

### **ğŸ¯ Trained Models:**
- **XGBoost Optimized**: F1-Score > 0.85, ROC-AUC > 0.90
- **LightGBM Optimized**: Fast training, high accuracy
- **Random Forest Optimized**: Robust ensemble predictions
- **Best Model Selection**: Automatically selects top performer

### **ğŸ“ˆ Comprehensive Reports:**
- **Model Comparison**: Performance metrics across all models
- **Feature Importance**: SHAP-based feature analysis
- **Business Metrics**: Recovery precision
- **Validation Results**: Crosidation and holdout testing

### **ğŸ¨ Interactive Dashboard:**
- **Real-time Predictions**: Upload CSV or input customer data
- **Model Explanations**: SHAP plots and feature importance
- **Business Insights**: Risk segmentation and recommendations
- **Performance Monitoring**: Model metrics and data drift

### **ğŸ” Explainability:**
- **SHAP Summary Plots**: Global feature importance
- **Individual Explanations**: Per-customer prediction reasoning
- **Feature Insights**: Business-relevant feature analysis

## ğŸ› ï¸ **DVC Commands Reference**

```bash
# C Commands
dvc repro                    # Run complete pipeline
dvc status                   # Check what needs to be run
dvc dag                      # Visualize pipeline structure

# Stage-specific Commands  
dvc repro data_generation    # Generate data only
dvc repro feature_engineering # Feature engineering only
dvc repro complete_pipeline  # ML training + dashboard only

# Metrics and Results
dvc metrics show             # Show all metrics
dvc metrics diff             # Compare metrics across runs
dvc plots show              # Generate performance plots

# Data Management
dvc push                     # Push data to remote storage
dvc pull                     # Pull data from remote storage
dvc checkout                 # Restore data to specific version

# Experiment Tracking
exp run                  # Run experiment with parameters
dvc exp show                 # Compare experiments
dvc exp diff                 # Show experiment differences
```

## ğŸ§ª **Advanced Usage**

### **Custom Parameters:**
```bash
# Modify pipeline parameters
dvc repro --set-param data_generation.n_samples=20000
dvc repro --set-param complete_pipeline.optimization_method=optuna
dvc repro --set-param complete_pipeline.n_trials=100
```

### **Experiment Tracking:**
```bash
# Run experiment with custom name
dvc exp run --name "large_dataset" --set-param data_generation.n_samples=50000

# Compare experiments
dvc exp show --include-metrics

# Apply best experiment
dvc exp apply exp-12345
```

### **Pipeline Debugging:**
```bash
# Run with verbose output
dvc repro --verbose

# Force re-run specific stage
dvc repro --force data_generation

# Dry run (show what would be executed)
dvc repro --dry
```
